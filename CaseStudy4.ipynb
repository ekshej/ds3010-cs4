{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 : Deep Learning\n",
    "\n",
    "** Due Date: February 24, 2020, BEFORE the beginning of class at 11:00am **\n",
    "\n",
    "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mapr.com/blog/demystifying-ai-ml-dl/assets/process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    member 1\n",
    "    \n",
    "    member 2\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired outcome of the case study.**\n",
    "* Similar to case study 3 we will look at movie reviews from the v2.0 polarity dataset comes from\n",
    "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies divided into positive and negative reviews.\n",
    " \n",
    "**NOTE:  This case study is, one purpose, more open ended than the previous case studies.\n",
    "* This is intended to help prepare you for the final case study which will be even more open-ended.\n",
    "    \n",
    "**Required Readings:** \n",
    "* This case study will be based upon the scikit-learn Python library\n",
    "* Again will build upon the turtorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* Read about deep learning at https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Same as case study 3, except for the extra credit question.\n",
    "    * Numpy (www.numpy.org) \n",
    "    * Matplotlib (matplotlib.org) \n",
    "    * Scikit-learn (scikit-learn.org) \n",
    "    * You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points): Load in the movie review data, create TF-IDF features, and use two of your favorite classification algorithms from sci-kit learn for predicting sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This problem is, basically, already answered as part of case study 3, so it is fine to use your work from there to help answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lover\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.02\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.85; std - 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.86      0.87       258\n",
      "         pos       0.86      0.87      0.86       242\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "[[223  35]\n",
      " [ 31 211]]\n"
     ]
    }
   ],
   "source": [
    "# import scikit-learn\n",
    "\n",
    "\"\"\"Build a sentiment analysis / polarity model\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess whether the opinion of the author is\n",
    "positive or negative.\n",
    "In this examples we will use a movie review dataset.\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
    "    # block to be able to use a multi-core grid search that also works under\n",
    "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
    "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
    "    # that is used when n_jobs != 1 in GridSearchCV\n",
    "\n",
    "    # the training data folder must be passed as first argument\n",
    "    movie_reviews_data_folder = sys.argv[1]\n",
    "    dataset = load_files('txt_sentoken', shuffle=False)\n",
    "    print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    grid_search.fit(docs_train, y_train)\n",
    "\n",
    "    # TASK: print the mean and std for each candidate along with the parameter\n",
    "    # settings for all the candidates explored by grid search.\n",
    "    n_candidates = len(grid_search.cv_results_['params'])\n",
    "    for i in range(n_candidates):\n",
    "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "                 % (grid_search.cv_results_['params'][i],\n",
    "                    grid_search.cv_results_['mean_test_score'][i],\n",
    "                    grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.matshow(cm)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs_train\n",
    "\n",
    "# high max and min results in only very frequent words\n",
    "vectorizer = TfidfVectorizer(max_df = 0.9, min_df = 0.1, ngram_range = (2,2))\n",
    "\n",
    "# low max and min results in much less frequent words\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = []\n",
    "Xtrain = []\n",
    "for word in vectorizer.get_feature_names():\n",
    "    feature_names.append(word.replace(\"_\", \"\"))\n",
    "Xtrain = list(set(feature_names))\n",
    "        \n",
    "# print(feature_names)\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217  41]\n",
      " [ 33 209]]\n",
      "[[201  57]\n",
      " [ 64 178]]\n"
     ]
    }
   ],
   "source": [
    "#Run two favored algorithms:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 0.0025)\n",
    "vectorizer_fit = vectorizer.fit(docs_train)\n",
    "\n",
    "Xtrain = vectorizer_fit.transform(docs_train)\n",
    "\n",
    "Xtest = vectorizer_fit.transform(docs_test)\n",
    "\n",
    "classifier_one = LinearSVC()\n",
    "classifier_one.fit(Xtrain, y_train)\n",
    "\n",
    "classifier_two = KNeighborsClassifier(n_neighbors=250\n",
    "                                      , weights=\"distance\")\n",
    "classifier_two.fit(Xtrain, y_train)\n",
    "\n",
    "prediction_one = classifier_one.predict(Xtest)\n",
    "prediction_two = classifier_two.predict(Xtest)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, prediction_one))\n",
    "print(metrics.confusion_matrix(y_test, prediction_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Use a Multi-Layer Perceptron (MLP) for classifying the reviews.  Explore the parameters for the MLP and compare the accuracies against your baseline algorithms in Problem 1.\n",
    "\n",
    "**Read the documentation for the MLPClassifier class at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.** \n",
    "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of accuracy?\n",
    "* Try different values for \"activation\". What do you observe in terms of accuracy?\n",
    "* Try different values for \"solver\". What do you observe in terms of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100,100,100))\n",
    "\n",
    "classifier.fit(Xtrain, y_train)\n",
    "y_pred = classifier.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
      " 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0\n",
      " 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1\n",
      " 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
      " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 1\n",
      " 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
      " 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (20 points): Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
    "**Compare the runtime of your  baseline algorithms to the runtime of the MLPClassifier** \n",
    "\n",
    "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
    "* Try different values for \"hidden_layer_sizes\".  What do you observe in term of runtime?\n",
    "* Try different values for \"activation\". What do you observe in term of runtime?\n",
    "* Try different values for \"solver\". What do you observe in term of runtime?\n",
    "* How long does the \"fit\" function take as opposed to the \"predict\" function?  Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Problem 4 (20 points): Business question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose you had a machine learning algorithms that could detect the sentinment of tweets that was highly accurate.  What kind of business could you build around that?\n",
    "* Who would be your competitors, and what are their sizes?\n",
    "* What would be the size of the market for your product?\n",
    "* In addition, assume that your machine learning was slow to train, but fast in making predicitions on new data.  How would that affect your business plan?\n",
    "* How could you use the cloud to support your product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 (extra credit): MLPs are not all there is!\n",
    "\n",
    "* In real life, MLPs are rarely is ever used for NLP. \n",
    "     * In this class I don't want you to have to learn too many different libraries and MLP is all that scikit-learn reall has.\n",
    "* What is a Recurrent Neural Network (RNN), and why might they be better for NLP?\n",
    "* What is a Convolutional Neural Network (CNN), and why might they be better for NLP?\n",
    "* Pytorch is a famous library for deep learning and complete tutorial for sentiment analysis using Pytorch can be found at https://github.com/bentrevett/pytorch-sentiment-analysis.\n",
    "    * What do they do for sentiment analysis?\n",
    "    * For a bit of fun, you can actually run their notebooks on Google-colab at the touch of a button!\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides (for 10 minutes of presentation) (20 points)\n",
    "\n",
    "\n",
    "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
    "\n",
    "2. (10 points) Communicating Results (figure/table)\n",
    "\n",
    "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "\n",
    "*Please compress all the files into a single zipped file.*\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu).\n",
    "\n",
    "#### We auto-process the submissions so make sure your subject line is *exactly*:\n",
    "\n",
    "### DS3010 Case Study 4 Team ??\n",
    "\n",
    "#### where ?? is your team number.\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
