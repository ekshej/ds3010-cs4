{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 : Deep Learning\n",
    "\n",
    "** Due Date: February 24, 2020, BEFORE the beginning of class at 11:00am **\n",
    "\n",
    "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mapr.com/blog/demystifying-ai-ml-dl/assets/process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    Achu Balasubramanian\n",
    "    \n",
    "    Irean Ali\n",
    "    \n",
    "    Josh Lovering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired outcome of the case study.**\n",
    "* Similar to case study 3 we will look at movie reviews from the v2.0 polarity dataset comes from\n",
    "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies divided into positive and negative reviews.\n",
    " \n",
    "**NOTE:  This case study is, one purpose, more open ended than the previous case studies.\n",
    "* This is intended to help prepare you for the final case study which will be even more open-ended.\n",
    "    \n",
    "**Required Readings:** \n",
    "* This case study will be based upon the scikit-learn Python library\n",
    "* Again will build upon the turtorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* Read about deep learning at https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Same as case study 3, except for the extra credit question.\n",
    "    * Numpy (www.numpy.org) \n",
    "    * Matplotlib (matplotlib.org) \n",
    "    * Scikit-learn (scikit-learn.org) \n",
    "    * You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points): Load in the movie review data, create TF-IDF features, and use two of your favorite classification algorithms from sci-kit learn for predicting sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This problem is, basically, already answered as part of case study 3, so it is fine to use your work from there to help answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Achu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Achu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.85; std - 0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.85      0.86       248\n",
      "         pos       0.86      0.88      0.87       252\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "[[212  36]\n",
      " [ 31 221]]\n"
     ]
    }
   ],
   "source": [
    "# import scikit-learn\n",
    "\n",
    "\"\"\"Build a sentiment analysis / polarity model\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess whether the opinion of the author is\n",
    "positive or negative.\n",
    "In this examples we will use a movie review dataset.\n",
    "\"\"\"\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# License: Simplified BSD\n",
    "\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
    "    # block to be able to use a multi-core grid search that also works under\n",
    "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
    "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
    "    # that is used when n_jobs != 1 in GridSearchCV\n",
    "\n",
    "    # the training data folder must be passed as first argument\n",
    "    movie_reviews_data_folder = sys.argv[1]\n",
    "    dataset = load_files('txt_sentoken', shuffle=False)\n",
    "    print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    grid_search.fit(docs_train, y_train)\n",
    "\n",
    "    # TASK: print the mean and std for each candidate along with the parameter\n",
    "    # settings for all the candidates explored by grid search.\n",
    "    n_candidates = len(grid_search.cv_results_['params'])\n",
    "    for i in range(n_candidates):\n",
    "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "                 % (grid_search.cv_results_['params'][i],\n",
    "                    grid_search.cv_results_['mean_test_score'][i],\n",
    "                    grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.matshow(cm)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs_train\n",
    "\n",
    "# high max and min results in only very frequent words\n",
    "vectorizer = TfidfVectorizer(max_df = 0.9, min_df = 0.1, ngram_range = (2,2))\n",
    "\n",
    "# low max and min results in much less frequent words\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = []\n",
    "Xtrain = []\n",
    "for word in vectorizer.get_feature_names():\n",
    "    feature_names.append(word.replace(\"_\", \"\"))\n",
    "Xtrain = list(set(feature_names))\n",
    "        \n",
    "# print(feature_names)\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[208  40]\n",
      " [ 36 216]]\n",
      "[[225  23]\n",
      " [ 90 162]]\n"
     ]
    }
   ],
   "source": [
    "#Run two favored algorithms:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 0.0025)\n",
    "vectorizer_fit = vectorizer.fit(docs_train)\n",
    "\n",
    "Xtrain = vectorizer_fit.transform(docs_train)\n",
    "\n",
    "Xtest = vectorizer_fit.transform(docs_test)\n",
    "\n",
    "classifier_one = LinearSVC()\n",
    "classifier_one.fit(Xtrain, y_train)\n",
    "\n",
    "classifier_two = KNeighborsClassifier(n_neighbors=250\n",
    "                                      , weights=\"distance\")\n",
    "classifier_two.fit(Xtrain, y_train)\n",
    "\n",
    "prediction_one = classifier_one.predict(Xtest)\n",
    "prediction_two = classifier_two.predict(Xtest)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, prediction_one))\n",
    "print(metrics.confusion_matrix(y_test, prediction_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear SVC: 74/500 false predictions\n",
    "\n",
    "K Neighbors: 121/500 false predictions\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Use a Multi-Layer Perceptron (MLP) for classifying the reviews.  Explore the parameters for the MLP and compare the accuracies against your baseline algorithms in Problem 1.\n",
    "\n",
    "**Read the documentation for the MLPClassifier class at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.** \n",
    "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of accuracy?\n",
    "* Try different values for \"activation\". What do you observe in terms of accuracy?\n",
    "* Try different values for \"solver\". What do you observe in terms of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer size: 50, Activation String: identity, Solver String: lbfgs\n",
      "[[212  26]\n",
      " [ 42 220]]\n",
      "Layer size: 50, Activation String: identity, Solver String: sgd\n",
      "[[221  17]\n",
      " [210  52]]\n",
      "Layer size: 50, Activation String: identity, Solver String: adam\n",
      "[[209  29]\n",
      " [ 43 219]]\n",
      "Layer size: 50, Activation String: logistic, Solver String: lbfgs\n",
      "[[212  26]\n",
      " [ 48 214]]\n",
      "Layer size: 50, Activation String: logistic, Solver String: sgd\n",
      "[[238   0]\n",
      " [262   0]]\n",
      "Layer size: 50, Activation String: logistic, Solver String: adam\n",
      "[[210  28]\n",
      " [ 41 221]]\n",
      "Layer size: 50, Activation String: tanh, Solver String: lbfgs\n",
      "[[214  24]\n",
      " [ 42 220]]\n",
      "Layer size: 50, Activation String: tanh, Solver String: sgd\n",
      "[[225  13]\n",
      " [237  25]]\n",
      "Layer size: 50, Activation String: tanh, Solver String: adam\n",
      "[[209  29]\n",
      " [ 42 220]]\n",
      "Layer size: 50, Activation String: relu, Solver String: lbfgs\n",
      "[[217  21]\n",
      " [ 44 218]]\n",
      "Layer size: 50, Activation String: relu, Solver String: sgd\n",
      "[[238   0]\n",
      " [262   0]]\n",
      "Layer size: 50, Activation String: relu, Solver String: adam\n",
      "[[208  30]\n",
      " [ 46 216]]\n",
      "Layer size: 100, Activation String: identity, Solver String: lbfgs\n",
      "[[215  23]\n",
      " [ 41 221]]\n",
      "Layer size: 100, Activation String: identity, Solver String: sgd\n",
      "[[214  24]\n",
      " [215  47]]\n",
      "Layer size: 100, Activation String: identity, Solver String: adam\n",
      "[[209  29]\n",
      " [ 44 218]]\n",
      "Layer size: 100, Activation String: logistic, Solver String: lbfgs\n",
      "[[218  20]\n",
      " [ 46 216]]\n",
      "Layer size: 100, Activation String: logistic, Solver String: sgd\n",
      "[[238   0]\n",
      " [262   0]]\n",
      "Layer size: 100, Activation String: logistic, Solver String: adam\n",
      "[[210  28]\n",
      " [ 43 219]]\n",
      "Layer size: 100, Activation String: tanh, Solver String: lbfgs\n",
      "[[214  24]\n",
      " [ 41 221]]\n",
      "Layer size: 100, Activation String: tanh, Solver String: sgd\n",
      "[[218  20]\n",
      " [220  42]]\n",
      "Layer size: 100, Activation String: tanh, Solver String: adam\n",
      "[[209  29]\n",
      " [ 43 219]]\n",
      "Layer size: 100, Activation String: relu, Solver String: lbfgs\n",
      "[[215  23]\n",
      " [ 44 218]]\n",
      "Layer size: 100, Activation String: relu, Solver String: sgd\n",
      "[[  6 232]\n",
      " [  1 261]]\n",
      "Layer size: 100, Activation String: relu, Solver String: adam\n",
      "[[209  29]\n",
      " [ 43 219]]\n",
      "Layer size: 200, Activation String: identity, Solver String: lbfgs\n",
      "[[214  24]\n",
      " [ 42 220]]\n",
      "Layer size: 200, Activation String: identity, Solver String: sgd\n",
      "[[238   0]\n",
      " [257   5]]\n",
      "Layer size: 200, Activation String: identity, Solver String: adam\n",
      "[[209  29]\n",
      " [ 43 219]]\n",
      "Layer size: 200, Activation String: logistic, Solver String: lbfgs\n",
      "[[214  24]\n",
      " [ 46 216]]\n",
      "Layer size: 200, Activation String: logistic, Solver String: sgd\n",
      "[[238   0]\n",
      " [262   0]]\n",
      "Layer size: 200, Activation String: logistic, Solver String: adam\n",
      "[[209  29]\n",
      " [ 42 220]]\n",
      "Layer size: 200, Activation String: tanh, Solver String: lbfgs\n",
      "[[217  21]\n",
      " [ 44 218]]\n",
      "Layer size: 200, Activation String: tanh, Solver String: sgd\n",
      "[[221  17]\n",
      " [218  44]]\n",
      "Layer size: 200, Activation String: tanh, Solver String: adam\n",
      "[[210  28]\n",
      " [ 42 220]]\n",
      "Layer size: 200, Activation String: relu, Solver String: lbfgs\n",
      "[[215  23]\n",
      " [ 49 213]]\n",
      "Layer size: 200, Activation String: relu, Solver String: sgd\n",
      "[[113 125]\n",
      " [125 137]]\n",
      "Layer size: 200, Activation String: relu, Solver String: adam\n",
      "[[209  29]\n",
      " [ 43 219]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layer_sizes = [50, 100, 200]\n",
    "activation_strings = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "solver_strings = ['lbfgs', 'sgd', 'adam']\n",
    "\n",
    "for layer in layer_sizes:\n",
    "    for activation_string in activation_strings:\n",
    "        for solver_string in solver_strings:\n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(layer,), activation = activation_string, solver = solver_string)\n",
    "            classifier.fit(Xtrain, y_train)\n",
    "            y_pred = classifier.predict(Xtest)\n",
    "            print(\"Layer size: \" + str(layer) + \", Activation String: \" + activation_string + \", Solver String: \" + solver_string)\n",
    "            print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We ran each possible parameter together, for hidden layer sizes we chose\n",
    "the three values: 50, 100, and 200.\n",
    "\n",
    "The top 4 values were:\n",
    "200 layers, identity, lbfgs: 65 false predictions\n",
    "100 layers, logistic, lbfgs: 72 false predictions\n",
    "50 layers, logistic, adam: 73 false predictions\n",
    "50 layers, relu, adam: 73 false predictions\n",
    "\n",
    "Hidden Layer Sizes: The more layers the better the predictions were on average.\n",
    "Though 50 layers resulted in 2 of the top 4 values. A lower layer size paired\n",
    "with the adam solver proves very successful, while in other cases layer sizes\n",
    "of 100 or 200 works best.\n",
    "\n",
    "Activations: Of the average false predictions, the order of most to least\n",
    "successful activations was: identity, relu, tanh, logistic. Though all of \n",
    "these activations had very similar average results. Relu works bester with\n",
    "smaller layer sizes. Identity paired with lbfgs works best with high layer sizes\n",
    "while identity paired with adam works best with lower layer sizes.\n",
    "\n",
    "Solvers: Of the average false predictions, the order of most to least\n",
    "successful solvers was: adam, lbfgs, sgd. sgd by far had the poorest results - it predicted a lot of false negatives. \n",
    "lbfgs works well with higher layer sizes and pairs bet with identity and logistic activations.\n",
    "adam works best with smaller layer sizes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (20 points): Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
    "**Compare the runtime of your  baseline algorithms to the runtime of the MLPClassifier** \n",
    "\n",
    "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
    "* Try different values for \"hidden_layer_sizes\".  What do you observe in term of runtime?\n",
    "* Try different values for \"activation\". What do you observe in term of runtime?\n",
    "* Try different values for \"solver\". What do you observe in term of runtime?\n",
    "* How long does the \"fit\" function take as opposed to the \"predict\" function?  Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.4 ms ± 4.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "5.81 ms ± 460 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "396 µs ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "227 ms ± 7.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Layer size: 50, Activation String: identity, Solver String: lbfgs\n",
      "3.65 s ± 724 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.6 ms ± 191 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: identity, Solver String: sgd\n",
      "9.23 s ± 3.02 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.74 ms ± 257 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: identity, Solver String: adam\n",
      "39.4 s ± 816 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "9.21 ms ± 1.47 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: logistic, Solver String: lbfgs\n",
      "6.4 s ± 974 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "9.14 ms ± 140 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: logistic, Solver String: sgd\n",
      "5.66 s ± 639 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.84 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: logistic, Solver String: adam\n",
      "1min 12s ± 2.01 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "9.61 ms ± 1.27 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: tanh, Solver String: lbfgs\n",
      "3.96 s ± 807 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.81 ms ± 192 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: tanh, Solver String: sgd\n",
      "The slowest run took 4.47 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "11 s ± 3.84 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.81 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: tanh, Solver String: adam\n",
      "39.4 s ± 980 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.83 ms ± 136 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: relu, Solver String: lbfgs\n",
      "5.33 s ± 1.18 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "9.61 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: relu, Solver String: sgd\n",
      "The slowest run took 4.42 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "9.46 s ± 3.53 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "8.87 ms ± 150 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 50, Activation String: relu, Solver String: adam\n",
      "38.6 s ± 793 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "9.59 ms ± 1.29 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: identity, Solver String: lbfgs\n",
      "7.02 s ± 752 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "20.3 ms ± 498 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 100, Activation String: identity, Solver String: sgd\n",
      "14.5 s ± 6.84 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "16.3 ms ± 180 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: identity, Solver String: adam\n",
      "59 s ± 790 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "18.4 ms ± 2.08 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: logistic, Solver String: lbfgs\n",
      "11.2 s ± 3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "17.6 ms ± 1.16 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: logistic, Solver String: sgd\n",
      "9.28 s ± 511 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "17.8 ms ± 1.16 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: logistic, Solver String: adam\n",
      "1min 53s ± 1.52 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "17 ms ± 279 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: tanh, Solver String: lbfgs\n",
      "6.99 s ± 414 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "16.9 ms ± 1.28 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: tanh, Solver String: sgd\n",
      "The slowest run took 4.50 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "19.7 s ± 6.8 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "20 ms ± 640 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: tanh, Solver String: adam\n",
      "59 s ± 443 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "18.8 ms ± 2.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 100, Activation String: relu, Solver String: lbfgs\n",
      "The slowest run took 6.22 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "9.62 s ± 4.23 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21.3 ms ± 449 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 100, Activation String: relu, Solver String: sgd\n",
      "9.86 s ± 5.12 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "16.3 ms ± 137 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 100, Activation String: relu, Solver String: adam\n",
      "58.1 s ± 573 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "16.3 ms ± 257 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Layer size: 200, Activation String: identity, Solver String: lbfgs\n",
      "12 s ± 2.25 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "28.1 ms ± 697 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: identity, Solver String: sgd\n",
      "33.9 s ± 9.28 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "33.4 ms ± 3.37 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: identity, Solver String: adam\n",
      "1min 41s ± 6.38 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "27.5 ms ± 665 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: logistic, Solver String: lbfgs\n",
      "26 s ± 3.84 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "30 ms ± 723 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: logistic, Solver String: sgd\n",
      "18.7 s ± 2.17 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "32.4 ms ± 2.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: logistic, Solver String: adam\n",
      "3min 22s ± 8.9 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "30.4 ms ± 605 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: tanh, Solver String: lbfgs\n",
      "13.4 s ± 1.27 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "30.4 ms ± 1.91 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: tanh, Solver String: sgd\n",
      "The slowest run took 4.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "29.4 s ± 16.5 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "30.2 ms ± 1.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: tanh, Solver String: adam\n",
      "1min 43s ± 4.31 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "28.6 ms ± 794 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: relu, Solver String: lbfgs\n",
      "17.4 s ± 1.52 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "28.8 ms ± 751 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: relu, Solver String: sgd\n",
      "18 s ± 5.74 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "29.7 ms ± 314 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Layer size: 200, Activation String: relu, Solver String: adam\n",
      "1min 37s ± 6.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "30.9 ms ± 1.81 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 0.0025)\n",
    "vectorizer_fit = vectorizer.fit(docs_train)\n",
    "\n",
    "\n",
    "Xtrain = vectorizer_fit.transform(docs_train)\n",
    "\n",
    "\n",
    "Xtest = vectorizer_fit.transform(docs_test)\n",
    "\n",
    "classifier_one = LinearSVC()\n",
    "%timeit classifier_one.fit(Xtrain, y_train)\n",
    "\n",
    "classifier_two = KNeighborsClassifier(n_neighbors=250\n",
    "                                      , weights=\"distance\")\n",
    "%timeit classifier_two.fit(Xtrain, y_train)\n",
    "\n",
    "%timeit classifier_one.predict(Xtest)\n",
    "%timeit classifier_two.predict(Xtest)\n",
    "\n",
    "layer_sizes = [50, 100, 200]\n",
    "activation_strings = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "solver_strings = ['lbfgs', 'sgd', 'adam']\n",
    "\n",
    "for layer in layer_sizes:\n",
    "    for activation_string in activation_strings:\n",
    "        for solver_string in solver_strings:\n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(layer,), activation = activation_string, solver = solver_string)\n",
    "            print(\"Layer size: \" + str(layer) + \", Activation String: \" + activation_string + \", Solver String: \" + solver_string)\n",
    "            %timeit classifier.fit(Xtrain, y_train)\n",
    "            %timeit classifier.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Overall, the baseline algorithms performed their operations much much faster than the deep learning\n",
    "algorithms, presumable due to the complexity in implementing the latter.\n",
    "\n",
    "Hidden Layer Sizes: The more layers the the longer the fitting took, on average. The\n",
    "run time performed better relative to the number of layers, however. \n",
    "\n",
    "Activations: Idenity, relu, and tanh all reported very similar fitting runtimes (around 30 seconds).\n",
    "Logistic was by far the slowest with an average of over 50 seconds.\n",
    "\n",
    "Solvers: Adam had by far the longest average run time, with the 12 slowest times all belonging to it.\n",
    "Lbfgs was the quickest and had good accuracy, while sgd was slightly slower but very inaccurate\n",
    "\n",
    "Fitting took much longer than predicting across all the measurements. This is probably due to the \n",
    "larger data sets that needed to be processed and the time consumed in creating a model.\n",
    "Something interesting to note is that prediction times were fairly consistent across all choices,\n",
    "only ranging from 1-3 milliseconds.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Problem 4 (20 points): Business question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose you had a machine learning algorithms that could detect the sentinment of tweets that was highly accurate.  What kind of business could you build around that?\n",
    "* Who would be your competitors, and what are their sizes?\n",
    "* What would be the size of the market for your product?\n",
    "* In addition, assume that your machine learning was slow to train, but fast in making predicitions on new data.  How would that affect your business plan?\n",
    "* How could you use the cloud to support your product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Business: Create an algorithm that does sentiment analysis on tweets regarding gym experience. \n",
    "This can be sold to a large chain gym with many locations. The gym can use this to monitor their locations and see if any\n",
    "are not being ran to their satisfaction and may be damaging their name. We would use keyword queries to find a list of tweets\n",
    "that mention the respective gym. We would then use further keyword searching and maybe geotags to determine which gym location\n",
    "is being tweeted about. Our algorithm would run on these list of tweets mentioning each location and determine which are positive\n",
    "and which are negative tweets. The gym locations with many negative tweets would be flagged and some of those tweets could be further\n",
    "analyzed to provide a reason for these poor gym experiences in order to give the company an actionable problem to fix.\n",
    "\n",
    "Competitors: A competitor of ours would be the gym's marketing team. This team would likely be in charge of managing their twitter\n",
    "account and boosting their branches' business. This team would report back any dissatisfied customers to the headquarters \n",
    "in order to notify the company of any flaws that could be fixed. This team may be larger than ours as this is likely a global\n",
    "or country wide gym company. Other competitors would be groups like ours with large training and testing sets and \n",
    "algorithms trained on sentiment analysis. Many of these competitors may be part of large companies and be of greater size \n",
    "than us. In order to combat their larger size we would gain access to more datasets to further train our algorithm. A downside\n",
    "is that larger groups can offord more manpower to sort and label datasets manually, so we would have to find/purchase datasets\n",
    "to be competetive with these groups.\n",
    "\n",
    "Market Size: English speaking countries. To expand the business we could incorportate translators or \n",
    "word sets including more languages\n",
    "\n",
    "Slow to train, fast to predict: We can take time to build the business and train the algorithm.\n",
    "then once we sell it each application runs quickly.\n",
    "\n",
    "Cloud Support: Store tons of training & testing datasets in order to make the algorithm very thorough. We could train our\n",
    "algorithm on movie reviews and other sentiment datasets to further expand its versatility in predicting tweets on \n",
    "different gym branches.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 (extra credit): MLPs are not all there is!\n",
    "\n",
    "* In real life, MLPs are rarely is ever used for NLP. \n",
    "     * In this class I don't want you to have to learn too many different libraries and MLP is all that scikit-learn reall has.\n",
    "* What is a Recurrent Neural Network (RNN), and why might they be better for NLP?\n",
    "* What is a Convolutional Neural Network (CNN), and why might they be better for NLP?\n",
    "* Pytorch is a famous library for deep learning and complete tutorial for sentiment analysis using Pytorch can be found at https://github.com/bentrevett/pytorch-sentiment-analysis.\n",
    "    * What do they do for sentiment analysis?\n",
    "    * For a bit of fun, you can actually run their notebooks on Google-colab at the touch of a button!\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RNN processes in sequence & time series information\n",
    "Ideal for speech and text\n",
    "Can be used for image captioning\n",
    "Difficult to train\n",
    "Takes a long time (exponential)\n",
    "Classification, Regression, Forecasting\n",
    "\n",
    "CNN is Supervised Machine Learning\n",
    "Requires large training data sets\n",
    "Uses 3 main layers:\n",
    "Convolution\n",
    "RELU\n",
    "Pooling\n",
    "CNN is inspired by the organization of the visual cortex\n",
    "CNN is a variation of MLP and uses minimal amounts of preprocessing\n",
    "Ideal for images and videos\n",
    "\n",
    "PyTorch\n",
    "Released in 2016\n",
    "Deep Neural Networks\n",
    "Tensor Computing\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides (for 10 minutes of presentation) (20 points)\n",
    "\n",
    "\n",
    "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
    "\n",
    "2. (10 points) Communicating Results (figure/table)\n",
    "\n",
    "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "\n",
    "*Please compress all the files into a single zipped file.*\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu).\n",
    "\n",
    "#### We auto-process the submissions so make sure your subject line is *exactly*:\n",
    "\n",
    "### DS3010 Case Study 4 Team ??\n",
    "\n",
    "#### where ?? is your team number.\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
